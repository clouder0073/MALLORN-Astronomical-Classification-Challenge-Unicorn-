import pandas as pd
import numpy as np
import extinction
from astropy.cosmology import Planck18 as cosmo
from astropy import units as u
import joblib
import os
from tqdm import tqdm
from scipy.optimize import curve_fit
from scipy.stats import theilslopes
import warnings
warnings.filterwarnings('ignore')

print("=== TDEæµ‹è¯•é›†é¢„æµ‹ ===")

# é…ç½®å‚æ•°
BASE_PATH = './data/'
EFF_WAVELENGTHS = {'u': 3641, 'g': 4704, 'r': 6155, 'i': 7504, 'z': 8695, 'y': 10056}
R_V = 3.1
FILTERS = ['u', 'g', 'r', 'i', 'z', 'y']

# 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
print("--- åŠ è½½æ¨¡å‹ ---")
model_info = joblib.load('optimized_tde_model.pkl')
selected_features = model_info['features']
best_threshold = model_info['best_threshold']
models = model_info['models']

print(f"æ¨¡å‹ç±»å‹: {model_info['strategy']}")
print(f"ç‰¹å¾æ•°é‡: {len(selected_features)}")
print(f"æœ€ä½³é˜ˆå€¼: {best_threshold:.3f}")
print(f"æ¨¡å‹æ•°é‡: {len(models)}")

# 2. åŠ è½½æµ‹è¯•é›†å…ƒæ•°æ®
print("\n--- åŠ è½½æµ‹è¯•é›†æ•°æ® ---")
test_log = pd.read_csv(f'{BASE_PATH}test_log.csv')
print(f"æµ‹è¯•é›†å¯¹è±¡æ•°é‡: {len(test_log)}")

# 3. æµ‹è¯•é›†ç‰¹å¾å·¥ç¨‹å‡½æ•°ï¼ˆä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´ï¼‰
def tde_decay_model(t, alpha, t0, A):
    return A * (t - t0 + 1e-6) ** (-alpha)

def fit_decay_alpha(group):
    group = group.sort_values('mjd')
    t = group['mjd'].values
    f = group['flux_corrected'].values
    if len(f) < 3: return np.nan
    peak_idx = np.argmax(f)
    if peak_idx == len(f) - 1: return np.nan
    t_decay = t[peak_idx:] - t[peak_idx]
    f_decay = f[peak_idx:]
    try:
        popt, _ = curve_fit(
            tde_decay_model, t_decay, f_decay,
            p0=[1.5, 0, f[peak_idx]],
            bounds=([0.5, -10, 0], [3.0, 10, np.inf]),
            maxfev=500
        )
        alpha = popt[0]
        return alpha if 0.5 <= alpha <= 3.0 else np.nan
    except:
        return np.nan

def is_single_peak(group):
    f = group['flux_corrected'].values
    if len(f) < 3: return 0
    peaks = (f[1:-1] > f[:-2]) & (f[1:-1] > f[2:])
    return int(np.sum(peaks) == 1)

def rise_fall_ratio(group):
    group = group.sort_values('mjd')
    t = group['mjd'].values
    f = group['flux_corrected'].values
    if len(f) < 3: return np.nan
    peak_idx = np.argmax(f)
    if peak_idx == 0 or peak_idx == len(f) - 1: return np.nan
    rise_time = t[peak_idx] - t[0]
    fall_time = t[-1] - t[peak_idx]
    return rise_time / (fall_time + 1e-6)

def extract_improved_features(df):
    """æå–æ”¹è¿›çš„ç¨³å¥ç‰¹å¾ï¼ˆä¸è®­ç»ƒé›†ç›¸åŒï¼‰"""
    features_list = []
    
    for object_id in df['object_id'].unique():
        obj_data = df[df['object_id'] == object_id].sort_values('mjd')
        
        features = {}
        flux = obj_data['flux_corrected'].values
        mjd = obj_data['mjd'].values
        
        # 1. è´Ÿæµé‡å¤„ç†
        flux_positive = flux.clip(min=1e-9)
        flux_abs = np.abs(flux)
        
        # åˆ†ä½æ•°ç‰¹å¾
        for q in [10, 25, 50, 75, 90]:
            features[f'flux_q{q}'] = np.percentile(flux_positive, q)
            features[f'flux_abs_q{q}'] = np.percentile(flux_abs, q)
        
        features['positive_ratio'] = (flux > 0).mean()
        features['flux_abs_median'] = np.median(flux_abs)
        
        # 2. è§‚æµ‹gapç‰¹å¾
        if len(mjd) > 1:
            mjd_diff = np.diff(mjd)
            features['gap_mean'] = np.mean(mjd_diff)
            features['gap_std'] = np.std(mjd_diff)
            features['gap_max'] = np.max(mjd_diff)
            features['gap_median'] = np.median(mjd_diff)
            features['obs_density'] = len(obj_data) / (mjd[-1] - mjd[0] + 1e-9)
        
        # 3. æ³¢æ®µç‰¹å¾
        unique_filters = obj_data['Filter'].nunique()
        features['filter_coverage'] = unique_filters / 6.0
        
        # å„æ³¢æ®µçš„è§‚æµ‹æ¬¡æ•°æ¯”ä¾‹
        for band in ['u', 'g', 'r', 'i', 'z', 'y']:
            band_count = (obj_data['Filter'] == band).sum()
            features[f'{band}_obs_ratio'] = band_count / len(obj_data)
        
        # 4. ä¿¡å™ªæ¯”ç‰¹å¾
        snr = flux_positive / (obj_data['flux_err'].values + 1e-9)
        features['snr_median'] = np.median(snr)
        features['snr_q10'] = np.percentile(snr, 10)
        features['snr_q90'] = np.percentile(snr, 90)
        
        # 5. æ”¹è¿›çš„è¶‹åŠ¿ç¨³å®šæ€§ï¼ˆä½¿ç”¨Theil-Senï¼‰
        if len(flux) >= 5:
            try:
                x = np.arange(len(flux))
                slope, _, _, _ = theilslopes(flux, x)
                features['trend_slope'] = slope
                # è¶‹åŠ¿ç¨³å®šæ€§ï¼šæ®‹å·®çš„ç›¸å¯¹å¤§å°
                predicted = slope * x + np.median(flux)
                residuals = flux - predicted
                features['trend_stability'] = 1.0 / (np.std(residuals) / (np.std(flux) + 1e-9) + 1e-9)
            except:
                features['trend_slope'] = 0
                features['trend_stability'] = 0
        else:
            features['trend_slope'] = 0
            features['trend_stability'] = 0
        
        # 6. é€šç”¨æ—¶åºå½¢æ€ç‰¹å¾ï¼ˆä¸ä¾èµ–æ ‡ç­¾ï¼‰
        if len(flux) >= 3:
            # å˜å¼‚æ€§æŒ‡æ•°
            features['variability_index'] = np.std(flux) / (np.mean(flux_positive) + 1e-9)
            
            # å³°å€¼æ€§
            features['peakiness'] = np.max(flux_positive) / (np.median(flux_positive) + 1e-9)
            
            # ä¸å¯¹ç§°æ€§
            peak_idx = np.argmax(flux)
            if 0 < peak_idx < len(flux) - 1:
                before_peak = flux[:peak_idx]
                after_peak = flux[peak_idx+1:]
                std_before = np.std(before_peak) if len(before_peak) > 1 else 0
                std_after = np.std(after_peak) if len(after_peak) > 1 else 0
                features['asymmetry'] = std_after / (std_before + 1e-9)
            else:
                features['asymmetry'] = 0
            
            # ä¸Šå‡ä¸‹é™æ¯”ç‡
            if len(flux) > 5:
                first_half = flux[:len(flux)//2]
                second_half = flux[len(flux)//2:]
                features['rise_fall_ratio_global'] = np.mean(first_half) / (np.mean(second_half) + 1e-9)
        
        features['object_id'] = object_id
        features_list.append(features)
    
    return pd.DataFrame(features_list)

# 4. å¤„ç†æµ‹è¯•é›†å…‰å˜æ•°æ®
def process_test_lightcurves():
    """å¤„ç†æµ‹è¯•é›†å…‰å˜æ•°æ®å¹¶æå–ç‰¹å¾"""
    
    print("\n--- å¤„ç†æµ‹è¯•é›†å…‰å˜æ•°æ® ---")
    
    # æ£€æŸ¥æµ‹è¯•é›†æ–‡ä»¶ç»“æ„
    test_lc_paths = []
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å•ä¸ªæµ‹è¯•é›†æ–‡ä»¶
    single_test_path = f'{BASE_PATH}test_lightcurves/test_full_lightcurves.csv'
    if os.path.exists(single_test_path):
        test_lc_paths = [single_test_path]
        print("æ‰¾åˆ°å•ä¸ªæµ‹è¯•é›†æ–‡ä»¶")
    else:
        # æ£€æŸ¥åˆ†splitçš„æµ‹è¯•é›†æ–‡ä»¶
        for i in range(1, 21):
            split_path = f'{BASE_PATH}split_{i:02d}/test_full_lightcurves.csv'
            if os.path.exists(split_path):
                test_lc_paths.append(split_path)
        print(f"æ‰¾åˆ° {len(test_lc_paths)} ä¸ªæµ‹è¯•é›†åˆ†ç‰‡æ–‡ä»¶")
    
    if not test_lc_paths:
        raise FileNotFoundError("æœªæ‰¾åˆ°æµ‹è¯•é›†å…‰å˜æ•°æ®æ–‡ä»¶")
    
    all_test_features = []
    
    for lc_path in tqdm(test_lc_paths, desc="å¤„ç†æµ‹è¯•é›†åˆ†ç‰‡"):
        # åŠ è½½å…‰å˜æ•°æ®
        lc = pd.read_csv(lc_path)
        lc.rename(columns={'Time (MJD)': 'mjd', 'Flux': 'flux', 'Flux_err': 'flux_err'}, inplace=True)
        
        # åªå¤„ç†åœ¨test_logä¸­å­˜åœ¨çš„object_id
        relevant_objects = test_log['object_id'].unique()
        lc = lc[lc['object_id'].isin(relevant_objects)]
        
        if lc.empty:
            continue
        
        processed_dfs = []
        
        # å¯¹æ¯ä¸ªobject_idè¿›è¡Œæµé‡æ ¡æ­£
        for object_id in lc['object_id'].unique():
            object_lc = lc[lc['object_id'] == object_id].copy()
            if object_lc.empty:
                continue
                
            # è·å–è¯¥å¯¹è±¡çš„EBVå€¼
            object_ebv = test_log[test_log['object_id'] == object_id]['EBV'].iloc[0]
            A_v = R_V * object_ebv
            
            flux_corrected_list = []
            for _, row in object_lc.iterrows():
                A_lambda = extinction.fitzpatrick99(
                    np.array([EFF_WAVELENGTHS[row['Filter']]]), A_v, R_V
                )[0]
                flux_corrected_list.append(row['flux'] * 10**(0.4 * A_lambda))
            
            object_lc['flux_corrected'] = flux_corrected_list
            processed_dfs.append(object_lc)
        
        if not processed_dfs:
            continue
            
        df = pd.concat(processed_dfs)
        
        # è®¡ç®—è·ç¦»å’Œç»å¯¹æ˜Ÿç­‰ï¼ˆä½¿ç”¨æµ‹è¯•é›†çš„Zï¼Œæ³¨æ„æµ‹è¯•é›†Zå¯èƒ½æœ‰è¯¯å·®ï¼‰
        df['Z'] = df['object_id'].map(test_log.set_index('object_id')['Z'])
        z_values = df['Z'].fillna(0).values
        z_values[z_values <= 0] = 1e-6
        
        # è®¡ç®—è·ç¦»
        dist_pc = cosmo.luminosity_distance(z_values).to(u.pc).value
        df['distance_pc'] = dist_pc
        
        # è®¡ç®—ç»å¯¹æ˜Ÿç­‰
        df['flux_positive'] = df['flux_corrected'].clip(lower=1e-9)
        df['apparent_mag'] = -2.5 * np.log10(df['flux_positive'])
        df['absolute_mag'] = df['apparent_mag'] - 5 * (np.log10(df['distance_pc']) - 1)
        
        # åŸºç¡€è®¡ç®—
        df['snr'] = df['flux_corrected'] / df['flux_err']
        df = df.sort_values(['object_id', 'mjd'])
        
        # ç‰¹å¾å·¥ç¨‹
        grouped = df.groupby('object_id')
        
        # åŸºç¡€ç‰¹å¾
        agg_features = grouped.agg(
            flux_mean=('flux_corrected', 'mean'),
            flux_std=('flux_corrected', 'std'),
            flux_max=('flux_corrected', 'max'),
            flux_min=('flux_corrected', 'min'),
            flux_skew=('flux_corrected', 'skew'),
            mjd_span=('mjd', lambda x: x.max() - x.min()),
            snr_mean=('snr', 'mean'),
            snr_max=('snr', 'max'),
            snr_std=('snr', 'std'),
            obs_count=('mjd', 'size')
        )
        
        # ç»å¯¹æ˜Ÿç­‰ç‰¹å¾
        abs_mag_agg = grouped.agg(
            abs_mag_min=('absolute_mag', 'min'),
            abs_mag_max=('absolute_mag', 'max'),
            abs_mag_mean=('absolute_mag', 'mean'),
            abs_mag_std=('absolute_mag', 'std'),
            abs_mag_span=('absolute_mag', lambda x: x.max() - x.min())
        )
        agg_features = agg_features.join(abs_mag_agg, how='left')
        
        # æ—¶é—´åŠ æƒæ–œç‡
        def weighted_slope(group):
            group = group.sort_values('mjd')
            delta_mjd = np.diff(group['mjd'])
            delta_flux = np.diff(group['flux_corrected'])
            slopes = delta_flux / (delta_mjd + 1e-9)
            return np.mean(slopes) if len(slopes) > 0 else 0
        
        agg_features['weighted_slope_mean'] = grouped.apply(weighted_slope)
        
        # ç‰©ç†é©±åŠ¨ç‰¹å¾
        agg_features['decay_alpha'] = grouped.apply(fit_decay_alpha)
        agg_features['is_single_peak'] = grouped.apply(is_single_peak)
        agg_features['rise_fall_ratio'] = grouped.apply(rise_fall_ratio)
        
        # åˆ†æ³¢æ®µç‰¹å¾
        pivot_mean = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='mean').add_suffix('_mean')
        pivot_std = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='std').add_suffix('_std')
        pivot_max = df.pivot_table(index='object_id', columns='Filter', values='flux_corrected', aggfunc='max').add_suffix('_max')
        agg_features = agg_features.join([pivot_mean, pivot_std, pivot_max], how='left')
        
        # é¢œè‰²ç‰¹å¾
        for f in FILTERS:
            if f'{f}_mean' not in agg_features.columns: 
                agg_features[f'{f}_mean'] = np.nan
        agg_features['color_g_r_mean'] = agg_features['g_mean'] - agg_features['r_mean']
        agg_features['color_r_i_mean'] = agg_features['r_mean'] - agg_features['i_mean']
        agg_features['color_i_z_mean'] = agg_features['i_mean'] - agg_features['z_mean']
        
        # æ–°å¢æ”¹è¿›ç‰¹å¾
        improved_features = extract_improved_features(df)
        agg_features = agg_features.merge(improved_features, on='object_id', how='left')
        
        all_test_features.append(agg_features)
    
    if not all_test_features:
        raise ValueError("æœªç”Ÿæˆä»»ä½•æµ‹è¯•é›†ç‰¹å¾")
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    test_feature_df = pd.concat(all_test_features)
    test_feature_df.reset_index(inplace=True)
    
    return test_feature_df

# 5. ç”Ÿæˆæµ‹è¯•é›†ç‰¹å¾
test_features_df = process_test_lightcurves()

# 6. åˆå¹¶å…ƒæ•°æ®å’Œç‰¹å¾
print("\n--- åˆå¹¶æµ‹è¯•é›†æ•°æ® ---")
test_final = test_log.merge(test_features_df, on='object_id', how='left')

# æ£€æŸ¥ç¼ºå¤±ç‰¹å¾
missing_features = set(selected_features) - set(test_final.columns)
if missing_features:
    print(f"è­¦å‘Š: ç¼ºå¤± {len(missing_features)} ä¸ªç‰¹å¾ï¼Œå°†ç”¨0å¡«å……")
    for feature in missing_features:
        test_final[feature] = 0

# 7. å‡†å¤‡é¢„æµ‹æ•°æ®
X_test = test_final[selected_features].copy()

# å¤„ç†ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„å¡«å……ç­–ç•¥ï¼‰
numeric_cols = X_test.select_dtypes(include=[np.number]).columns
X_test[numeric_cols] = X_test[numeric_cols].fillna(0)

std_cols = [c for c in X_test.columns if 'std' in c]
X_test[std_cols] = X_test[std_cols].fillna(0)

print(f"æµ‹è¯•é›†ç‰¹å¾å½¢çŠ¶: {X_test.shape}")

# 8. è¿›è¡Œé¢„æµ‹
print("\n--- è¿›è¡Œé¢„æµ‹ ---")
test_predictions = []

for i, model in enumerate(models):
    pred = model.predict_proba(X_test)[:, 1]
    test_predictions.append(pred)
    print(f"æ¨¡å‹ {i+1} é¢„æµ‹å®Œæˆ")

# å¹³å‡é¢„æµ‹
test_preds_mean = np.mean(test_predictions, axis=0)

# åº”ç”¨æœ€ä½³é˜ˆå€¼
test_binary = (test_preds_mean >= best_threshold).astype(int)

print(f"æµ‹è¯•é›†é¢„æµ‹å®Œæˆ:")
print(f"å¹³å‡é¢„æµ‹æ¦‚ç‡: {test_preds_mean.mean():.4f} Â± {test_preds_mean.std():.4f}")
print(f"æ­£æ ·æœ¬é¢„æµ‹æ¯”ä¾‹: {test_binary.mean():.4f}")
print(f"é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:")
for percentile in [10, 25, 50, 75, 90]:
    value = np.percentile(test_preds_mean, percentile)
    print(f"  {percentile}%åˆ†ä½æ•°: {value:.4f}")

# 9. ç”Ÿæˆæäº¤æ–‡ä»¶
print("\n--- ç”Ÿæˆæäº¤æ–‡ä»¶ ---")
submission = pd.DataFrame({
    'object_id': test_final['object_id'],
    'predicted': test_binary
})

# ç¡®ä¿æ‰€æœ‰æµ‹è¯•é›†å¯¹è±¡éƒ½æœ‰é¢„æµ‹
missing_objects = set(test_log['object_id']) - set(submission['object_id'])
if missing_objects:
    print(f"è­¦å‘Š: {len(missing_objects)} ä¸ªå¯¹è±¡æ²¡æœ‰é¢„æµ‹ï¼Œå°†è®¾ä¸º0")
    missing_df = pd.DataFrame({
        'object_id': list(missing_objects),
        'predicted': 0
    })
    submission = pd.concat([submission, missing_df], ignore_index=True)

# æŒ‰object_idæ’åº
submission = submission.sort_values('object_id')

# ä¿å­˜æäº¤æ–‡ä»¶
submission_file = 'submission.csv'
submission.to_csv(submission_file, index=False)

print(f"âœ… æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: '{submission_file}'")
print(f"æäº¤æ–‡ä»¶å½¢çŠ¶: {submission.shape}")
print(f"æ­£æ ·æœ¬é¢„æµ‹æ•°é‡: {submission['predicted'].sum()}")
print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {submission['predicted'].mean():.4f}")

# 10. ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
detailed_predictions = pd.DataFrame({
    'object_id': test_final['object_id'],
    'prediction_prob': test_preds_mean,
    'predicted': test_binary
})
detailed_predictions.to_csv('test_detailed_predictions.csv', index=False)
print("è¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜: 'test_detailed_predictions.csv'")

# 11. é¢„æµ‹åˆ†æ
print("\n--- é¢„æµ‹åˆ†æ ---")
print(f"ä½¿ç”¨çš„é˜ˆå€¼: {best_threshold}")
print(f"é¢„æµ‹æ¦‚ç‡èŒƒå›´: [{test_preds_mean.min():.4f}, {test_preds_mean.max():.4f}]")
print(f"é«˜ç½®ä¿¡åº¦æ­£æ ·æœ¬ (æ¦‚ç‡ > 0.7): {(test_preds_mean > 0.7).sum()}")
print(f"é«˜ç½®ä¿¡åº¦è´Ÿæ ·æœ¬ (æ¦‚ç‡ < 0.3): {(test_preds_mean < 0.3).sum()}")

# æ£€æŸ¥ç‰¹å¾é‡è¦æ€§ä¸€è‡´æ€§
if hasattr(models[0], 'feature_importances_'):
    feature_importance = np.mean([model.feature_importances_ for model in models], axis=0)
    importance_df = pd.DataFrame({
        'feature': selected_features,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print("\nTop 10æœ€é‡è¦ç‰¹å¾åœ¨æµ‹è¯•é›†çš„è¡¨ç°:")
    print(importance_df.head(10))

print("\nğŸ‰ æµ‹è¯•é›†é¢„æµ‹å®Œæˆï¼")
print("ğŸ“¤ è¯·æäº¤ 'submission.csv' åˆ°Kaggle")
print(f"ğŸ“Š é¢„æœŸLBåˆ†æ•°åº”è¯¥æ¥è¿‘: {model_info['oof_score']:.4f} (åŸºäºOOF F1)")
print("ğŸ’¡ å¦‚æœLBåˆ†æ•°ä¸ç†æƒ³ï¼Œè¯·æ£€æŸ¥æµ‹è¯•é›†ä¸è®­ç»ƒé›†çš„åˆ†å¸ƒå·®å¼‚")