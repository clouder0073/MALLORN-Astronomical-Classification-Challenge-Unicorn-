import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import GroupKFold
from sklearn.metrics import f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import joblib
import warnings
warnings.filterwarnings('ignore')

print("=== TDEåˆ†ç±»æ¨¡å‹ç²¾ç»†è°ƒä¼˜ ===")

# 1. åŠ è½½æ•°æ®
print("--- åŠ è½½æ•°æ® ---")
df = pd.read_csv('processed_train_features_improved.csv')
features_to_exclude = ['object_id', 'SpecType', 'English Translation', 'split', 'target']
all_features = [col for col in df.columns if col not in features_to_exclude]
X = df[all_features]
y = df['target']
groups = df['split']

print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.4f}")

# 2. åŸºäºä¹‹å‰ç»“æœé€‰æ‹©æœ€ä½³ç‰¹å¾å­é›†
def select_optimal_features(df):
    """åŸºäºä¹‹å‰å®éªŒé€‰æ‹©æœ€ä½³ç‰¹å¾"""
    
    # ä»ä¹‹å‰ç»“æœä¸­è¡¨ç°å¥½çš„ç‰¹å¾
    high_importance_features = [
        'flux_abs_q25', 'flux_abs_q10', 'decay_alpha', 'r_max', 'mjd_span',
        'Z', 'flux_abs_q50', 'u_mean', 'i_max', 'u_max', 'peakiness',
        'color_r_i_mean', 'g_max', 'abs_mag_mean', 'abs_mag_min',
        'abs_mag_max', 'rise_fall_ratio', 'rise_fall_ratio_global',
        'asymmetry', 'u_std', 'positive_ratio', 'flux_abs_median',
        'variability_index', 'trend_stability', 'filter_coverage',
        'gap_max', 'obs_density', 'snr_mean', 'obs_count', 'flux_mean'
    ]
    
    # åªé€‰æ‹©å­˜åœ¨çš„ç‰¹å¾
    selected = [f for f in high_importance_features if f in df.columns]
    print(f"é€‰æ‹©äº† {len(selected)} ä¸ªé«˜é‡è¦æ€§ç‰¹å¾")
    return selected

print("\n--- ç‰¹å¾é€‰æ‹© ---")
selected_features = select_optimal_features(df)
X_optimal = df[selected_features]

# 3. å¤šç­–ç•¥æ¨¡å‹è®­ç»ƒ
def train_multiple_strategies(X, y, groups):
    """å°è¯•å¤šç§è®­ç»ƒç­–ç•¥"""
    
    strategies = {}
    pos_weight = len(y) / (2 * np.sum(y))
    
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    
    # ç­–ç•¥1: ä¸­ç­‰æ­£åˆ™åŒ–
    print("\n--- ç­–ç•¥1: ä¸­ç­‰æ­£åˆ™åŒ– ---")
    models1, oof1, scores1 = train_strategy(X, y, groups, {
        'n_estimators': 1500,
        'learning_rate': 0.05,
        'num_leaves': 31,
        'max_depth': 7,
        'min_child_samples': 20,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.3,
        'reg_lambda': 0.5,
        'scale_pos_weight': pos_weight
    }, "ä¸­ç­‰æ­£åˆ™åŒ–")
    
    strategies['medium_reg'] = {
        'models': models1, 
        'oof_preds': oof1, 
        'scores': scores1,
        'params': 'ä¸­ç­‰æ­£åˆ™åŒ–'
    }
    
    # ç­–ç•¥2: è½»åº¦æ­£åˆ™åŒ– + æ—©åœ
    print("\n--- ç­–ç•¥2: è½»åº¦æ­£åˆ™åŒ– + ä¸¥æ ¼æ—©åœ ---")
    models2, oof2, scores2 = train_strategy(X, y, groups, {
        'n_estimators': 1000,
        'learning_rate': 0.1,
        'num_leaves': 63,
        'max_depth': 8,
        'min_child_samples': 10,
        'subsample': 0.9,
        'colsample_bytree': 0.9,
        'reg_alpha': 0.1,
        'reg_lambda': 0.2,
        'scale_pos_weight': pos_weight
    }, "è½»åº¦æ­£åˆ™åŒ–", early_stopping_rounds=50)
    
    strategies['light_reg'] = {
        'models': models2, 
        'oof_preds': oof2, 
        'scores': scores2,
        'params': 'è½»åº¦æ­£åˆ™åŒ–'
    }
    
    # ç­–ç•¥3: é›†æˆå­¦ä¹  (LightGBM + RandomForest)
    print("\n--- ç­–ç•¥3: æ¨¡å‹é›†æˆ ---")
    ensemble_preds = np.zeros(len(X))
    ensemble_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
        print(f"é›†æˆè®­ç»ƒ Fold {fold + 1}")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # LightGBM
        lgb_model = lgb.LGBMClassifier(
            n_estimators=500,
            learning_rate=0.05,
            num_leaves=31,
            max_depth=6,
            random_state=42 + fold,
            verbosity=-1
        )
        lgb_model.fit(X_train, y_train)
        lgb_preds = lgb_model.predict_proba(X_val)[:, 1]
        
        # RandomForest
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42 + fold,
            n_jobs=-1
        )
        rf_model.fit(X_train, y_train)
        rf_preds = rf_model.predict_proba(X_val)[:, 1]
        
        # å¹³å‡é¢„æµ‹
        ensemble_pred = (lgb_preds + rf_preds) / 2
        ensemble_preds[val_idx] = ensemble_pred
        
        # è®¡ç®—è¯¥foldçš„F1
        best_f1_fold = 0
        for thresh in np.arange(0.1, 0.9, 0.02):
            f1 = f1_score(y_val, (ensemble_pred >= thresh).astype(int))
            if f1 > best_f1_fold:
                best_f1_fold = f1
        
        ensemble_scores.append(best_f1_fold)
        print(f"Fold {fold + 1} F1: {best_f1_fold:.4f}")
    
    strategies['ensemble'] = {
        'models': None,  # é›†æˆæ¨¡å‹ä¸ä¿å­˜å•ä¸ªæ¨¡å‹
        'oof_preds': ensemble_preds, 
        'scores': ensemble_scores,
        'params': 'LightGBM+RFé›†æˆ'
    }
    
    return strategies

def train_strategy(X, y, groups, params, strategy_name, early_stopping_rounds=100):
    """å•ä¸ªç­–ç•¥çš„è®­ç»ƒ"""
    
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    models = []
    oof_preds = np.zeros(len(X))
    fold_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):
        print(f"{strategy_name} - Fold {fold + 1}")
        
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model = lgb.LGBMClassifier(**params, verbosity=-1, n_jobs=-1)
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            eval_metric='binary_logloss',
            callbacks=[
                lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False),
                lgb.log_evaluation(period=0)
            ]
        )
        
        val_preds = model.predict_proba(X_val)[:, 1]
        oof_preds[val_idx] = val_preds
        
        # å¯»æ‰¾æœ€ä½³é˜ˆå€¼
        best_f1_fold = 0
        for thresh in np.arange(0.1, 0.9, 0.02):
            f1 = f1_score(y_val, (val_preds >= thresh).astype(int))
            if f1 > best_f1_fold:
                best_f1_fold = f1
        
        fold_scores.append(best_f1_fold)
        models.append(model)
        print(f"Fold {fold + 1} F1: {best_f1_fold:.4f}")
    
    return models, oof_preds, fold_scores

# 4. è¯„ä¼°æ‰€æœ‰ç­–ç•¥
def evaluate_strategies(strategies, y):
    """è¯„ä¼°æ‰€æœ‰è®­ç»ƒç­–ç•¥"""
    
    results = []
    best_strategy = None
    best_f1 = 0
    
    for name, strategy in strategies.items():
        oof_preds = strategy['oof_preds']
        
        # é˜ˆå€¼ä¼˜åŒ–
        best_threshold = 0.5
        best_f1_score = 0
        for thresh in np.arange(0.1, 0.9, 0.01):
            f1 = f1_score(y, (oof_preds >= thresh).astype(int))
            if f1 > best_f1_score:
                best_f1_score = f1
                best_threshold = thresh
        
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        binary_preds = (oof_preds >= best_threshold).astype(int)
        precision = np.mean(y[binary_preds == 1]) if np.sum(binary_preds) > 0 else 0
        recall = np.mean(binary_preds[y == 1])
        
        # è¿‡æ‹Ÿåˆåˆ†æï¼ˆä»…å¯¹å•ä¸ªæ¨¡å‹ç­–ç•¥ï¼‰
        if strategy['models'] is not None:
            train_f1 = analyze_overfitting(strategy['models'], X_optimal, y, best_threshold)
            overfitting_gap = train_f1 - best_f1_score
        else:
            train_f1 = np.nan
            overfitting_gap = np.nan
        
        results.append({
            'strategy': name,
            'params': strategy['params'],
            'oof_f1': best_f1_score,
            'precision': precision,
            'recall': recall,
            'threshold': best_threshold,
            'train_f1': train_f1,
            'overfitting_gap': overfitting_gap,
            'fold_scores': strategy['scores']
        })
        
        if best_f1_score > best_f1:
            best_f1 = best_f1_score
            best_strategy = name
    
    return pd.DataFrame(results), best_strategy

def analyze_overfitting(models, X, y, threshold):
    """åˆ†æè¿‡æ‹Ÿåˆ"""
    train_preds = []
    for model in models:
        train_pred = model.predict_proba(X)[:, 1]
        train_preds.append(train_pred)
    
    train_preds_mean = np.mean(train_preds, axis=0)
    return f1_score(y, (train_preds_mean >= threshold).astype(int))

# 5. ä¸»è®­ç»ƒæµç¨‹
print("\n--- å¼€å§‹å¤šç­–ç•¥è®­ç»ƒ ---")
strategies = train_multiple_strategies(X_optimal, y, groups)

print("\n--- ç­–ç•¥è¯„ä¼° ---")
results_df, best_strategy_name = evaluate_strategies(strategies, y)

print("\n=== æ‰€æœ‰ç­–ç•¥ç»“æœ ===")
for _, row in results_df.iterrows():
    print(f"\n{row['strategy']} ({row['params']}):")
    print(f"  OOF F1: {row['oof_f1']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {row['precision']:.4f}")
    print(f"  å¬å›ç‡: {row['recall']:.4f}")
    print(f"  é˜ˆå€¼: {row['threshold']:.3f}")
    if not pd.isna(row['overfitting_gap']):
        print(f"  è¿‡æ‹Ÿåˆå·®è·: {row['overfitting_gap']:.4f}")
    print(f"  å„Fold F1: {[f'{s:.4f}' for s in row['fold_scores']]}")

print(f"\nğŸ¯ æœ€ä½³ç­–ç•¥: {best_strategy_name}")
best_strategy = strategies[best_strategy_name]

# 6. æœ€ç»ˆæ¨¡å‹ä¼˜åŒ–
print("\n--- æœ€ç»ˆæ¨¡å‹ä¼˜åŒ– ---")

# å¦‚æœæœ€ä½³ç­–ç•¥æ˜¯é›†æˆå­¦ä¹ ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è®­ç»ƒä¸€ä¸ªå¯ä¿å­˜çš„ç‰ˆæœ¬
if best_strategy_name == 'ensemble':
    print("æœ€ä½³ç­–ç•¥æ˜¯é›†æˆå­¦ä¹ ï¼Œè®­ç»ƒå¯ä¿å­˜çš„ç‰ˆæœ¬...")
    
    # è®­ç»ƒæœ€ç»ˆçš„LightGBMæ¨¡å‹ï¼Œä½¿ç”¨æœ€ä½³å‚æ•°
    final_models = []
    gkf = GroupKFold(n_splits=min(5, groups.nunique()))
    
    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_optimal, y, groups)):
        print(f"æœ€ç»ˆè®­ç»ƒ Fold {fold + 1}")
        
        X_train, X_val = X_optimal.iloc[train_idx], X_optimal.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        model = lgb.LGBMClassifier(
            n_estimators=1000,
            learning_rate= 0.05 if best_strategy_name == 'medium_reg' else 0.1,
            num_leaves= 31 if best_strategy_name == 'medium_reg' else 63,
            max_depth= 7 if best_strategy_name == 'medium_reg' else 8,
            min_child_samples= 20 if best_strategy_name == 'medium_reg' else 10,
            subsample= 0.8 if best_strategy_name == 'medium_reg' else 0.9,
            colsample_bytree= 0.8 if best_strategy_name == 'medium_reg' else 0.9,
            reg_alpha= 0.3 if best_strategy_name == 'medium_reg' else 0.1,
            reg_lambda= 0.5 if best_strategy_name == 'medium_reg' else 0.2,
            scale_pos_weight= len(y) / (2 * np.sum(y)),
            random_state=42 + fold,
            verbosity=-1,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train)
        final_models.append(model)
    
    best_models = final_models
else:
    best_models = best_strategy['models']

# 7. ä¿å­˜æœ€ä½³æ¨¡å‹
print("\n--- ä¿å­˜æœ€ä½³æ¨¡å‹ ---")
best_model_info = {
    'models': best_models,
    'features': selected_features,
    'best_threshold': results_df[results_df['strategy'] == best_strategy_name]['threshold'].iloc[0],
    'oof_score': results_df[results_df['strategy'] == best_strategy_name]['oof_f1'].iloc[0],
    'strategy': best_strategy_name,
    'feature_names': selected_features
}

joblib.dump(best_model_info, 'optimized_tde_model.pkl')

# ä¿å­˜è¯¦ç»†ç»“æœ
results_df.to_csv('training_strategies_comparison.csv', index=False)

# ä¿å­˜OOFé¢„æµ‹
best_oof_preds = best_strategy['oof_preds']
oof_results = pd.DataFrame({
    'object_id': df['object_id'],
    'true_target': y,
    'oof_prediction': best_oof_preds,
    'oof_binary': (best_oof_preds >= best_model_info['best_threshold']).astype(int)
})
oof_results.to_csv('optimized_oof_predictions.csv', index=False)

print("âœ… ç²¾ç»†è°ƒä¼˜å®Œæˆ!")
print(f"ğŸ“Š æœ€ä½³ç­–ç•¥: {best_strategy_name}")
print(f"ğŸ¯ æœ€ä½³OOF F1: {best_model_info['oof_score']:.4f}")
print(f"âš–ï¸  æœ€ä½³é˜ˆå€¼: {best_model_info['best_threshold']:.3f}")
print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜: 'optimized_tde_model.pkl'")
print(f"ğŸ“ ç­–ç•¥æ¯”è¾ƒå·²ä¿å­˜: 'training_strategies_comparison.csv'")

# 8. å…³é”®æ´å¯Ÿ
print("\n--- å…³é”®æ´å¯Ÿ ---")
print("å½“å‰é—®é¢˜åˆ†æ:")
print("1. ä¸¥é‡è¿‡æ‹Ÿåˆ - éœ€è¦æ‰¾åˆ°æ­£åˆ™åŒ–å’Œæ¨¡å‹å¤æ‚åº¦çš„å¹³è¡¡ç‚¹")
print("2. æ•°æ®ä¸å¹³è¡¡ - æ­£æ ·æœ¬åªæœ‰148ä¸ªï¼Œéœ€è¦æ›´å¥½çš„é‡‡æ ·ç­–ç•¥")
print("3. ç‰¹å¾è´¨é‡ - éœ€è¦ç¡®ä¿ç‰¹å¾å…·æœ‰åˆ¤åˆ«åŠ›")

print("\nå»ºè®®ä¸‹ä¸€æ­¥:")
if best_model_info['oof_score'] < 0.38:
    print("âš ï¸  OOF F1ä»ç„¶è¾ƒä½ï¼Œå»ºè®®:")
    print("   - é‡æ–°æ£€æŸ¥ç‰¹å¾å·¥ç¨‹")
    print("   - å°è¯•è¿‡é‡‡æ ·æŠ€æœ¯(SMOTE)")
    print("   - è€ƒè™‘ä½¿ç”¨ç¥ç»ç½‘ç»œ")
else:
    print("âœ… ç»“æœå¯æ¥å—ï¼Œå¯ä»¥è¿›è¡Œæµ‹è¯•é›†é¢„æµ‹")